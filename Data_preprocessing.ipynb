{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\E.Sahin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\E.Sahin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Dataset Size: 221850 entries\n",
      "Entries removed by topic filtering: 67968\n",
      "Remaining entries: 153882\n",
      "\n",
      "Overactive authors identified: 406\n",
      "Entries removed from overactive authors: 30525\n",
      "Remaining entries: 123357\n",
      "\n",
      "URL-only entries removed: 376\n",
      "Remaining entries: 122981\n",
      "\n",
      "Reference-only entries removed: 4066\n",
      "Remaining entries: 118915\n",
      "\n",
      "Entries removed by length filtering: 11483\n",
      "Remaining entries: 107432\n",
      "\n",
      "Final Summary:\n",
      "Total entries removed: 114418\n",
      "Final dataset size: 107432\n",
      "\n",
      "Datasets saved as:\n",
      "- filtered_ekşi_sozluk_bert.csv\n",
      "- filtered_ekşi_sozluk_traditional_ml.csv\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset\n",
    "json_file_name = r\"C:\\Users\\E.Sahin\\Downloads\\eksi_party_entries\\eksi_party_entries.jsonl\"\n",
    "df = pd.read_json(json_file_name, lines=True, encoding='utf-8')\n",
    "initial_count = len(df)\n",
    "print(f\"Initial Dataset Size: {initial_count} entries\")\n",
    "\n",
    "# Topic filtering\n",
    "selected_topics = ['recep tayyip erdoğan', 'kemal kılıçdaroğlu']\n",
    "df_filtered = df[df['topic'].isin(selected_topics)]\n",
    "topics_removed = initial_count - len(df_filtered)\n",
    "print(f\"Entries removed by topic filtering: {topics_removed}\")\n",
    "print(f\"Remaining entries: {len(df_filtered)}\")\n",
    "\n",
    "# Remove overactive authors - Fixed grouping logic\n",
    "# First, count entries per author per topic\n",
    "author_counts = df_filtered.groupby(['topic', 'author_id']).size().reset_index(name='entry_count')\n",
    "\n",
    "# Calculate 99th percentile of entry counts per topic\n",
    "percentiles = author_counts.groupby('topic')['entry_count'].quantile(0.99)\n",
    "\n",
    "# Identify overactive authors\n",
    "overactive_authors = []\n",
    "for topic in selected_topics:\n",
    "    topic_threshold = percentiles[topic]\n",
    "    topic_overactive = author_counts[\n",
    "        (author_counts['topic'] == topic) & \n",
    "        (author_counts['entry_count'] > topic_threshold)\n",
    "    ]['author_id'].tolist()\n",
    "    overactive_authors.extend(topic_overactive)\n",
    "\n",
    "# Convert to unique integer array\n",
    "overactive_authors = np.unique(overactive_authors).astype(int)\n",
    "\n",
    "before_overactive = len(df_filtered)\n",
    "df_filtered = df_filtered[~df_filtered['author_id'].isin(overactive_authors)]\n",
    "overactive_removed = before_overactive - len(df_filtered)\n",
    "print(f\"\\nOveractive authors identified: {len(overactive_authors)}\")\n",
    "print(f\"Entries removed from overactive authors: {overactive_removed}\")\n",
    "print(f\"Remaining entries: {len(df_filtered)}\")\n",
    "\n",
    "# Remove URL-only entries\n",
    "before_urls = len(df_filtered)\n",
    "url_pattern = re.compile(r'^https?:\\/\\/\\S+$')\n",
    "df_filtered = df_filtered[~df_filtered['entry_text'].apply(lambda x: bool(url_pattern.match(str(x))))]\n",
    "urls_removed = before_urls - len(df_filtered)\n",
    "print(f\"\\nURL-only entries removed: {urls_removed}\")\n",
    "print(f\"Remaining entries: {len(df_filtered)}\")\n",
    "\n",
    "# Remove reference-only entries\n",
    "before_refs = len(df_filtered)\n",
    "reference_pattern = re.compile(r'^\\(bkz: .*?\\)$')\n",
    "df_filtered = df_filtered[~df_filtered['entry_text'].apply(lambda x: bool(reference_pattern.match(str(x))))]\n",
    "refs_removed = before_refs - len(df_filtered)\n",
    "print(f\"\\nReference-only entries removed: {refs_removed}\")\n",
    "print(f\"Remaining entries: {len(df_filtered)}\")\n",
    "\n",
    "# Length filtering\n",
    "before_length = len(df_filtered)\n",
    "df_filtered['entry_length'] = df_filtered['entry_text'].apply(lambda x: len(str(x)))\n",
    "df_filtered = df_filtered[(df_filtered['entry_length'] >= 35) & (df_filtered['entry_length'] <= 1407)]\n",
    "length_removed = before_length - len(df_filtered)\n",
    "print(f\"\\nEntries removed by length filtering: {length_removed}\")\n",
    "print(f\"Remaining entries: {len(df_filtered)}\")\n",
    "\n",
    "# Text preprocessing\n",
    "stop_words = set(stopwords.words('turkish'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the input text by performing the following steps:\n",
    "    1. Lowercasing the text.\n",
    "    2. Replacing '$' with 'ş' when adjacent to word characters.\n",
    "    3. Removing URLs.\n",
    "    4. Removing punctuation.\n",
    "    5. Removing stop words.\n",
    "    6. Lemmatizing the words.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The input text to preprocess.\n",
    "    \n",
    "    Returns:\n",
    "    str: The preprocessed text.\n",
    "    \"\"\"\n",
    "    # 1. Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    \n",
    "    # 7. Replace 'bkz:' with desired replacement (e.g., remove it)\n",
    "    text = re.sub(r'bkz:', '', text)\n",
    "    \n",
    "    # 2. Replace '$' with 'ş' if adjacent to a word character\n",
    "    # This ensures that only '$' within words are replaced\n",
    "    text = re.sub(r'(?<=\\w)\\$|\\$(?=\\w)', 'ş', text)\n",
    "    \n",
    "    # 3. Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # 4. Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 5. Remove stop words\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    # 6. Lemmatize the words\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "\n",
    "    return text\n",
    "\n",
    "# Create processed text column\n",
    "df_filtered['processed_text'] = df_filtered['entry_text'].apply(preprocess_text)\n",
    "\n",
    "# Remove the entry_length column\n",
    "df_filtered = df_filtered.drop('entry_length', axis=1)\n",
    "\n",
    "# Prepare final datasets\n",
    "bert_dataset = df_filtered[['entry_id', 'entry_date', 'topic', 'author_id', 'entry_text']]\n",
    "traditional_ml_dataset = df_filtered[['entry_id', 'entry_date','topic', 'author_id', 'entry_text','processed_text']]\n",
    "\n",
    "# Save datasets as Excel files\n",
    "bert_dataset.to_excel('filtered_ekşi_sozluk_bert.xlsx', index=False)\n",
    "traditional_ml_dataset.to_excel('filtered_ekşi_sozluk_traditional_ml.xlsx', index=False)\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\nFinal Summary:\")\n",
    "print(f\"Total entries removed: {initial_count - len(df_filtered)}\")\n",
    "print(f\"Final dataset size: {len(df_filtered)}\")\n",
    "print(\"\\nDatasets saved as:\")\n",
    "print(\"- filtered_ekşi_sozluk_bert.csv\")\n",
    "print(\"- filtered_ekşi_sozluk_traditional_ml.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_id</th>\n",
       "      <th>entry_date</th>\n",
       "      <th>topic</th>\n",
       "      <th>author_id</th>\n",
       "      <th>entry_text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9145023</td>\n",
       "      <td>2006-02-17 19:22:00</td>\n",
       "      <td>recep tayyip erdoğan</td>\n",
       "      <td>8099</td>\n",
       "      <td>me$gul ettigi makamin gerekli gordugu vasiflar...</td>\n",
       "      <td>meşgul ettigi makamin gerekli gordugu vasiflar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10006296</td>\n",
       "      <td>2006-05-09 21:38:00</td>\n",
       "      <td>recep tayyip erdoğan</td>\n",
       "      <td>8099</td>\n",
       "      <td>kendisine cok seri laflar hazirladigim adam. i...</td>\n",
       "      <td>kendisine cok seri laflar hazirladigim adam iş...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>49236290</td>\n",
       "      <td>2015-02-17 09:41:00</td>\n",
       "      <td>recep tayyip erdoğan</td>\n",
       "      <td>8099</td>\n",
       "      <td>her beyanını işittiğimde aklıma die hard'ın so...</td>\n",
       "      <td>beyanını işittiğimde aklıma die hardın sonlari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>52082664</td>\n",
       "      <td>2015-08-06 11:58:00</td>\n",
       "      <td>recep tayyip erdoğan</td>\n",
       "      <td>8099</td>\n",
       "      <td>hakkında bu kadar entry girilince konuştu sand...</td>\n",
       "      <td>hakkında kadar entry girilince konuştu sandım ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>123740137</td>\n",
       "      <td>2021-05-25 05:54:00</td>\n",
       "      <td>recep tayyip erdoğan</td>\n",
       "      <td>8099</td>\n",
       "      <td>hakkındaki piyon olduğu iddiası putin'i hatırl...</td>\n",
       "      <td>hakkındaki piyon olduğu iddiası putini hatırla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    entry_id           entry_date                 topic  author_id  \\\n",
       "3    9145023  2006-02-17 19:22:00  recep tayyip erdoğan       8099   \n",
       "5   10006296  2006-05-09 21:38:00  recep tayyip erdoğan       8099   \n",
       "7   49236290  2015-02-17 09:41:00  recep tayyip erdoğan       8099   \n",
       "8   52082664  2015-08-06 11:58:00  recep tayyip erdoğan       8099   \n",
       "9  123740137  2021-05-25 05:54:00  recep tayyip erdoğan       8099   \n",
       "\n",
       "                                          entry_text  \\\n",
       "3  me$gul ettigi makamin gerekli gordugu vasiflar...   \n",
       "5  kendisine cok seri laflar hazirladigim adam. i...   \n",
       "7  her beyanını işittiğimde aklıma die hard'ın so...   \n",
       "8  hakkında bu kadar entry girilince konuştu sand...   \n",
       "9  hakkındaki piyon olduğu iddiası putin'i hatırl...   \n",
       "\n",
       "                                      processed_text  \n",
       "3  meşgul ettigi makamin gerekli gordugu vasiflar...  \n",
       "5  kendisine cok seri laflar hazirladigim adam iş...  \n",
       "7  beyanını işittiğimde aklıma die hardın sonlari...  \n",
       "8  hakkında kadar entry girilince konuştu sandım ...  \n",
       "9  hakkındaki piyon olduğu iddiası putini hatırla...  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traditional_ml_dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  topic  entry_count\n",
      "0    kemal kılıçdaroğlu        50113\n",
      "1  recep tayyip erdoğan        57319\n"
     ]
    }
   ],
   "source": [
    "entries_by_topic = bert_dataset.groupby('topic').size().reset_index(name='entry_count')\n",
    "print(entries_by_topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
